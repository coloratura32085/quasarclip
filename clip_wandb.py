import argparse
import os
import torch
import pytorch_lightning as pl
from datasets import load_from_disk
from torch.utils.data import DataLoader
from datetime import datetime  # [æ–°å¢] ç”¨äºç”Ÿæˆæ—¶é—´æˆ³

from dataset_util.PairDataset import PairDataset
from imageutil.trans import CustomRandomHorizontalFlip, CustomRandomRotation, CustomRandomVerticalFlip, CustomCenterCrop, CustomExpStretchWithOffset, CustomRandom
from models.clip_resnet18 import AstroClipModel
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint
from pytorch_lightning import loggers as pl_loggers
from dataset_util.SpecDataset import SpecDataset
from root_path import ROOT_PATH
from specutil.scheduler import CosineAnnealingWithWarmupLR
import torchvision.transforms as transforms

# å¯¼å…¥ wandb
import wandb
from pytorch_lightning.loggers import WandbLogger

# è®¾ç½® Wandb API Key
# ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()
# è®¾ç½®éšæœºç§å­
pl.seed_everything(42)

# [ä¿®æ”¹] æ”¹ä¸ºæ¥æ”¶ size å‚æ•°ï¼Œä»¥æ”¯æŒåŠ¨æ€ Crop Size
class FlattenAndReshape:
    def __init__(self, size):
        self.size = size

    def __call__(self, x):
        # original_shape = x.shape
        x_flat = x.flatten()
        # ä½¿ç”¨ self.size åŠ¨æ€ reshape (ä¾‹å¦‚ 32x32x5 æˆ– 64x64x5)
        x_reshaped = x_flat.view(self.size, self.size, 5)
        x_final = x_reshaped.permute(2, 0, 1)
        return x_final

# è§£æå‘½ä»¤è¡Œå‚æ•°
def parse_args():
    parser = argparse.ArgumentParser(description="CLIP Training Script")

    # [æ–°å¢] åŠ¨æ€ Crop Size å‚æ•°ï¼Œé»˜è®¤ä¸º 32
    parser.add_argument('--crop_size', type=int, default=32, help="Crop size for transforms")

    # æ•°æ®è·¯å¾„
    parser.add_argument('--train_data_path', type=str, default=f'../data/data_g3_z/train_dataset',
                        help="Path to the training dataset")
    parser.add_argument('--test_data_path', type=str, default=f'../data/data_g3_z/test_dataset',
                        help="Path to the test dataset")

    # ä¼˜åŒ–å™¨è¶…å‚æ•°
    parser.add_argument('--lr', type=float, default=1e-3, help="Learning rate")
    parser.add_argument('--weight_decay', type=float, default=1e-3, help="Weight decay")
    parser.add_argument('--T_max', type=int, default=10_000, help="T_max for cosine annealing scheduler")
    parser.add_argument('--T_warmup', type=int, default=1_000, help="T_warmup for cosine annealing scheduler")

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=64, help="Batch size")
    parser.add_argument('--max_epochs', type=int, default=500, help="Number of epochs")
    parser.add_argument('--limit_val_batches', type=int, default=100, help="Limit validation batches")
    parser.add_argument('--gradient_clip_val', type=float, default=1.0, help="Gradient clipping value")

    # è¾“å‡ºè·¯å¾„ (ä½œä¸ºæ‰€æœ‰å®éªŒçš„æ ¹ç›®å½•)
    parser.add_argument('--output_dir', type=str, default=f'{ROOT_PATH}/outputs/clip/resnet_g3',
                        help="Root Output directory for checkpoints and logs")

    # å›¾åƒå’Œå…‰è°±ç¼–ç å™¨æƒé‡è·¯å¾„
    parser.add_argument('--spec_weight_path', type=str,
                        default=f'../outputs/spec/spec_g3/logs/lightning_logs/version_0/checkpoints/last.ckpt',
                        help="Path to image encoder weights")
    
    # wandb å‚æ•°
    parser.add_argument('--wandb_project', type=str, default='astro-clip', help="W&B project name")
    parser.add_argument('--wandb_name', type=str, default=None, help="W&B run name")
    parser.add_argument('--wandb_offline', action='store_true', help="Run W&B in offline mode")

    return parser.parse_args()


# è®­ç»ƒæ¨¡å‹
class ClipLightning(pl.LightningModule):
    def __init__(self, lr, weight_decay, T_max, T_warmup, spec_weight_path):
        super(ClipLightning, self).__init__()
        self.save_hyperparameters()

        self.model = AstroClipModel(spec_weight_path=spec_weight_path)

    def training_step(self, batch, batch_idx):
        loss_withlogit, loss_nologit, logit_scale = self.model.training_step(batch, batch_idx)
        self.log("train_loss_withlogit", loss_withlogit, on_epoch=True, prog_bar=True)
        self.log("train_loss_nologit", loss_nologit, on_epoch=True, prog_bar=True)
        self.log("scale", logit_scale)
        return loss_withlogit

    def validation_step(self, batch, batch_idx):
        val_loss_nologit, val_loss_withlogit = self.model.validation_step(batch, batch_idx)
        self.log("val_loss_nologit", val_loss_nologit, on_epoch=True, prog_bar=True)
        self.log("val_loss_withlogit", val_loss_withlogit, on_epoch=True, prog_bar=True)

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.hparams.lr,
            weight_decay=self.hparams.weight_decay,
        )

        scheduler = CosineAnnealingWithWarmupLR(
            optimizer,
            T_max=self.hparams.T_max,
            T_warmup=self.hparams.T_warmup,
            eta_min=self.hparams.lr / 500
        )

        return [optimizer], [scheduler]


def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()

    # ================= [é€»è¾‘ä¿®æ”¹: ç”Ÿæˆå”¯ä¸€å®éªŒè·¯å¾„] =================
    # è·å–å½“å‰æ—¶é—´
    current_time = datetime.now().strftime("%m%d_%H%M")
    
    # æ„é€ å®éªŒåç§° (Run Name)
    if args.wandb_name:
        run_name = f"{args.wandb_name}_{current_time}"
    else:
        # è‡ªåŠ¨å‘½å: clip_crop32_lr0.001_1208_1830
        run_name = f"clip_crop{args.crop_size}_lr{args.lr}_{current_time}"
    
    print(f"ğŸš€ Starting Experiment: {run_name}")

    # æ„é€ è¯¥æ¬¡å®éªŒçš„ä¸“å±ç›®å½•
    experiment_dir = os.path.join(args.output_dir, run_name)
    os.makedirs(experiment_dir, exist_ok=True)
    # ============================================================

    # åŠ è½½æ•°æ®é›†
    train_dataset = load_from_disk(args.train_data_path)
    test_dataset = load_from_disk(args.test_data_path)

    # [ä¿®æ”¹] åº”ç”¨åŠ¨æ€ Crop Size
    transform = transforms.Compose([
        CustomCenterCrop(size=args.crop_size),   # ä½¿ç”¨ä¼ å…¥çš„ crop_size
        FlattenAndReshape(size=args.crop_size),  # ä½¿ç”¨ä¼ å…¥çš„ crop_size
    ])

    # åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†å®ä¾‹
    train_dataset = PairDataset(train_dataset, transform=transform)
    test_dataset = PairDataset(test_dataset, transform=transform)

    # åˆ›å»º DataLoader
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=1)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=1)

    # åˆå§‹åŒ–æ¨¡å‹
    model = ClipLightning(lr=args.lr, weight_decay=args.weight_decay,
                          T_max=args.T_max, T_warmup=args.T_warmup,
                          spec_weight_path=args.spec_weight_path)

    # åˆå§‹åŒ– WandbLogger
    wandb_logger = WandbLogger(
        project=args.wandb_project,
        name=run_name,                # Wandb ç½‘é¡µæ˜¾ç¤ºçš„åç§°
        offline=args.wandb_offline,
        log_model=True,
        save_dir=experiment_dir,      # [å…³é”®] Wandb æ—¥å¿—å°†ä¿å­˜åœ¨ä¸“å±æ–‡ä»¶å¤¹å†…
        version=run_name              # å¼ºåˆ¶æœ¬åœ°æ–‡ä»¶å¤¹åä¸ run_name ä¸€è‡´
    )
    
    # è®°å½•é…ç½®åˆ° wandb
    wandb_logger.log_hyperparams({
        "architecture": "CLIP-ResNet18",
        "dataset": "astro-g3",
        "crop_size": args.crop_size,
        "experiment_dir": experiment_dir
    })

    # è®¾ç½®è®­ç»ƒå™¨å‚æ•°
    trainer = pl.Trainer(
        log_every_n_steps=16,
        default_root_dir=experiment_dir, # [å…³é”®] Checkpoint é»˜è®¤æ ¹ç›®å½•
        enable_checkpointing=True,
        gradient_clip_val=args.gradient_clip_val,
        max_epochs=args.max_epochs,
        limit_val_batches=args.limit_val_batches,
        logger=wandb_logger,
        callbacks=[
            LearningRateMonitor(logging_interval='step'),
            ModelCheckpoint(
                dirpath=os.path.join(experiment_dir, "checkpoints"), # [å…³é”®] æ˜¾å¼æŒ‡å®š ckpt ä¿å­˜è·¯å¾„
                monitor="val_loss_nologit",
                save_top_k=2,
                save_last=True,
                every_n_epochs=1,
                mode="min",
                # [å…³é”®] åŠ¨æ€æ–‡ä»¶å: epoch_005-val_loss_0.1234.ckpt
                filename='epoch_{epoch:03d}-val_loss_{val_loss_nologit:.4f}',
                auto_insert_metric_name=False
            ),
        ],
        strategy='ddp',
        accelerator='gpu',
        devices=[4,5,6], # ä¿æŒåŸæœ‰çš„æ˜¾å¡ ID
    )

    # ä½¿ç”¨ Trainer è¿›è¡Œè®­ç»ƒ
    trainer.fit(model, train_loader, test_loader)
    
    # è®­ç»ƒå®Œæˆåå…³é—­ wandb
    wandb.finish()

if __name__ == '__main__':
    main()
